<!-- static/index.html -->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hume AI Voice Chat</title>
    <style>
        body { font-family: sans-serif; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; background-color: #f0f0f0; }
        #controls button { font-size: 1.2em; padding: 10px 20px; cursor: pointer; border-radius: 8px; border: 1px solid #ccc; }
        #controls button:disabled { cursor: not-allowed; background-color: #e0e0e0; }
        #log { width: 80%; max-width: 800px; height: 400px; border: 1px solid #ccc; overflow-y: scroll; padding: 10px; background-color: #fff; margin-top: 20px; border-radius: 8px; }
        .user-message { color: blue; }
        .assistant-message { color: green; }
        .emotions { font-style: italic; color: #555; font-size: 0.9em; margin-top: -10px; padding-left: 15px; }
    </style>
</head>
<body>
    <h1>Hume AI Empathic Voice</h1>
    <div id="controls">
        <button id="connectButton">Start Conversation</button>
    </div>
    <div id="log"></div>

    <script>
        const connectButton = document.getElementById('connectButton');
        const logDiv = document.getElementById('log');

        // State management
        let socket;
        let audioContext;
        let processor;
        let mediaStream;
        let audioQueue = [];
        let isPlaying = false;
        const TARGET_SAMPLE_RATE = 24000;

        // --- Helper Functions for Audio Processing ---

        function downsampleBuffer(buffer, inputSampleRate, outputSampleRate) {
            if (inputSampleRate === outputSampleRate) return buffer;
            const sampleRateRatio = inputSampleRate / outputSampleRate;
            const newLength = Math.round(buffer.length / sampleRateRatio);
            const result = new Float32Array(newLength);
            let offsetResult = 0;
            let offsetBuffer = 0;
            while (offsetResult < result.length) {
                const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
                let accum = 0, count = 0;
                for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                    accum += buffer[i];
                    count++;
                }
                result[offsetResult] = accum / count;
                offsetResult++;
                offsetBuffer = nextOffsetBuffer;
            }
            return result;
        }

        function float32ToInt16(buffer) {
            let l = buffer.length;
            const buf = new Int16Array(l);
            while (l--) {
                buf[l] = Math.min(1, buffer[l]) * 0x7FFF;
            }
            return buf.buffer;
        }

        // --- UI and Logging ---

        function logMessage(type, role, content, emotions = null) {
            const messageElement = document.createElement('p');
            messageElement.className = type === 'user_message' ? 'user-message' : 'assistant-message';
            messageElement.innerHTML = `<strong>${role}:</strong> ${content}`;
            logDiv.appendChild(messageElement);

            if (emotions) {
                const emotionsElement = document.createElement('p');
                emotionsElement.className = 'emotions';
                const topEmotions = Object.entries(emotions)
                    .sort(([, a], [, b]) => b - a)
                    .slice(0, 3)
                    .map(([name, score]) => `${name} (${score.toFixed(2)})`)
                    .join(' | ');
                emotionsElement.textContent = `Top Emotions: ${topEmotions}`;
                logDiv.appendChild(emotionsElement);
            }
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        // --- Main Application Logic ---

        connectButton.onclick = () => {
            if (connectButton.textContent === 'Start Conversation') {
                startConversation();
            } else {
                stopConversation();
            }
        };

        async function startConversation() {
            logDiv.innerHTML = 'Connecting...';
            connectButton.textContent = 'Connecting...';
            connectButton.disabled = true;

            const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${wsProtocol}//${window.location.host}/ws`;
            socket = new WebSocket(wsUrl);

            socket.onopen = async () => {
                console.log('WebSocket connected.');
                logDiv.innerHTML = 'Connected! Please grant microphone access.';
                try {
                    await setupMicrophone();
                    connectButton.textContent = 'Stop Conversation';
                    connectButton.disabled = false;
                    logDiv.innerHTML = 'Microphone is active. Start speaking!';
                } catch (error) {
                    console.error('Error accessing microphone:', error);
                    logMessage('error', 'System', 'Could not access microphone. Please check permissions.');
                    if (socket) socket.close();
                }
            };

            socket.onmessage = (event) => {
                const message = JSON.parse(event.data);
                console.log('Received message:', message);

                if (message.type === 'user_message' || message.type === 'assistant_message') {
                    logMessage(message.type, message.message.role, message.message.content, message.models?.prosody?.scores);
                } else if (message.type === 'audio_output') {
                    const audioData = atob(message.data); // data is base64 encoded WAV
                    const audioBytes = new Uint8Array(audioData.length);
                    for (let i = 0; i < audioData.length; i++) {
                        audioBytes[i] = audioData.charCodeAt(i);
                    }
                    audioQueue.push(audioBytes.buffer);
                    playAudioQueue();
                } else if (message.type === 'error') {
                    console.error('Hume Error:', message.message);
                    logMessage('error', 'System', `Error: ${message.message}`);
                }
            };

            socket.onclose = () => {
                console.log('WebSocket disconnected.');
                stopConversation(); // Clean up everything
                logDiv.innerHTML += '<p>Connection closed.</p>';
            };

            socket.onerror = (error) => {
                console.error('WebSocket error:', error);
                logMessage('error', 'System', 'A connection error occurred.');
                stopConversation();
            };
        }

        async function setupMicrophone() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            const source = audioContext.createMediaStreamSource(mediaStream);

            // This AudioWorklet processor is the magic. It captures raw audio samples.
            const processorBlob = new Blob([`
                class AudioProcessor extends AudioWorkletProcessor {
                    process(inputs) {
                        const input = inputs[0];
                        if (input.length > 0) {
                            this.port.postMessage(input[0]);
                        }
                        return true;
                    }
                }
                registerProcessor('audio-processor', AudioProcessor);
            `], { type: 'application/javascript' });

            const processorUrl = URL.createObjectURL(processorBlob);
            await audioContext.audioWorklet.addModule(processorUrl);
            processor = new AudioWorkletNode(audioContext, 'audio-processor');
            source.connect(processor);
            processor.port.onmessage = (event) => {
                const pcmFloat32Data = event.data;
                const resampledData = downsampleBuffer(pcmFloat32Data, audioContext.sampleRate, TARGET_SAMPLE_RATE);
                const pcm16Data = float32ToInt16(resampledData);
                if (socket && socket.readyState === WebSocket.OPEN) {
                    socket.send(pcm16Data);
                }
            };
        }

        function stopConversation() {
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (socket && socket.readyState === WebSocket.OPEN) {
                socket.close();
            }
            socket = null;
            connectButton.textContent = 'Start Conversation';
            connectButton.disabled = false;
        }

        async function playAudioQueue() {
            if (isPlaying || audioQueue.length === 0) return;
            isPlaying = true;

            const audioBuffer = audioQueue.shift();
            const tempAudioContext = new (window.AudioContext || window.webkitAudioContext)();
            const decodedAudio = await tempAudioContext.decodeAudioData(audioBuffer);

            const source = tempAudioContext.createBufferSource();
            source.buffer = decodedAudio;
            source.connect(tempAudioContext.destination);
            source.start(0);

            source.onended = () => {
                tempAudioContext.close();
                isPlaying = false;
                playAudioQueue(); // Play next chunk
            };
        }
    </script>
</body>
</html>